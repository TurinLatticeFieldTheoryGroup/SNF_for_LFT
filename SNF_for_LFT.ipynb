{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtoB-sIktZ_E"
      },
      "source": [
        "# Stochastic Normalizing Flows for Lattice Field Theory\n",
        "Simple implementation of the Stochastic Normalizing Flows used in: <a href=\"https://link.springer.com/article/10.1007/JHEP07(2022)015\">Stochastic Normalizing Flows as non equilibrium transformations</a>\n",
        "\n",
        "Tutorial on standard Normalizing Flows: [Introduction to Normalizing Flows for Lattice Field Theory](https://arxiv.org/abs/2101.08176)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If this code is helpful for your research, please cite us:\n",
        "```\n",
        "@article{caselle2022stochastic,\n",
        "  title={Stochastic normalizing flows as non-equilibrium transformations},\n",
        "  author={Caselle, Michele and Cellini, Elia and Nada, Alessandro and Panero, Marco},\n",
        "  journal={Journal of High Energy Physics},\n",
        "  volume={2022},\n",
        "  number={7},\n",
        "  pages={1--31},\n",
        "  year={2022},\n",
        "  publisher={Springer}\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FAtEaiDbzevp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_j0XKLXwOrG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import gc\n",
        "import base64\n",
        "import io\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "from scipy.special import logsumexp\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sXGXGYFwYm0",
        "outputId": "70cf8959-20ed-4c23-bd98-5b869ee47a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TORCH DEVICE: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch_device = 'cuda'\n",
        "    float_dtype = np.float32 # single\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    torch_device = 'cpu'\n",
        "    float_dtype = np.float64 # double\n",
        "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "print(f\"TORCH DEVICE: {torch_device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zTC8A8g5qrg",
        "outputId": "c1912e97-868a-43ae-9eb2-3f8a578295a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35_KNOGgwYp8"
      },
      "outputs": [],
      "source": [
        "#Utility taken from https://arxiv.org/abs/2101.08176\n",
        "def grab(var):\n",
        "    if torch.is_tensor(var):\n",
        "        return var.detach().cpu().numpy()\n",
        "    else:\n",
        "        return var\n",
        "\n",
        "def init_live_plot(nstep, dpi=125, figsize=(8,4)):\n",
        "    fig, ax_ess = plt.subplots(1,1, dpi=dpi, figsize=figsize)\n",
        "    plt.xlim(0, nstep)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    ess_line = plt.plot([0],[0], alpha=0.5) # dummy\n",
        "    plt.grid(False)\n",
        "    plt.ylabel('ESS')\n",
        "\n",
        "    ax_loss = ax_ess.twinx()\n",
        "    loss_line = plt.plot([0],[0], alpha=0.5, c='orange') # dummy\n",
        "    plt.grid(False)\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "\n",
        "    display_id = display(fig, display_id=True)\n",
        "\n",
        "    return dict(\n",
        "        fig=fig, ax_ess=ax_ess, ax_loss=ax_loss,\n",
        "        ess_line=ess_line, loss_line=loss_line,\n",
        "        display_id=display_id\n",
        "    )\n",
        "\n",
        "def moving_average(x, window=10):\n",
        "    if len(x) < window:\n",
        "        return np.mean(x, keepdims=True)\n",
        "    else:\n",
        "        return np.convolve(x, np.ones(window), 'valid') / window\n",
        "\n",
        "def update_plots(history, fig, ax_ess, ax_loss, ess_line, loss_line, display_id):\n",
        "    Y = np.array(history['ess'])\n",
        "    Y = moving_average(Y, window=15)\n",
        "    ess_line[0].set_ydata(Y)\n",
        "    ess_line[0].set_xdata(np.arange(len(Y)))\n",
        "    Y = history['loss']\n",
        "    Y = moving_average(Y, window=15)\n",
        "    loss_line[0].set_ydata(np.array(Y))\n",
        "    loss_line[0].set_xdata(np.arange(len(Y)))\n",
        "    ax_loss.relim()\n",
        "    ax_loss.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    display_id.update(fig) # need to force colab to update plot\n",
        "\n",
        "def print_metrics(history, era, epoch, avg_last_N_epochs):\n",
        "    print(f'== Era {era} | Epoch {epoch} metrics ==')\n",
        "    for key, val in history.items():\n",
        "        avgd = np.mean(val[-avg_last_N_epochs:])\n",
        "        print(f'\\t{key} {avgd:g}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHuI6wTRYBIj"
      },
      "source": [
        "Target theory with action:\n",
        "$$S[\\phi]=\\sum_{x\\in\\Lambda}-2\\kappa \\sum_{\\mu=1,2}\\phi(x)\\phi(x+\\hat{\\mu})+(1-2\\lambda)\\phi(x)^2+\\lambda\\phi(x)^4$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ErvfUpwhbI"
      },
      "outputs": [],
      "source": [
        "class ScalarPhi4Action:\n",
        "    def __init__(self, kappa, lam, phi2coeff=0):\n",
        "        self.kappa = kappa\n",
        "        self.lam = lam\n",
        "        self.dims = range(1,3)\n",
        "        if (phi2coeff == 0):\n",
        "            self.phi2coeff = (1.0 - 2.0 * self.lam)\n",
        "        else:\n",
        "            self.phi2coeff = phi2coeff\n",
        "        self.info='_ScalarPhi4_kappa='+str(kappa)+'_lambda='+str(lam)\n",
        "\n",
        "    def __call__(self, cfgs, protocol=[1.,1.,1.]):\n",
        "        return torch.sum(self.action_density(cfgs), dim=tuple(self.dims))\n",
        "\n",
        "    def action_density(self, cfgs, protocol=[1.,1.,1.]):\n",
        "        return self.phi2term(cfgs) + self.phi4term(cfgs) + self.kineticterm(cfgs)\n",
        "\n",
        "    def phi2term(self, cfgs):\n",
        "        return self.phi2coeff * cfgs**2\n",
        "\n",
        "    def phi4term(self, cfgs):\n",
        "        return self.lam * cfgs**4\n",
        "\n",
        "    def kineticterm(self, cfgs):\n",
        "        action_density = cfgs * (torch.roll(cfgs, 1, 1) + torch.roll(cfgs, 1, 2))\n",
        "        return -2.0 * self.kappa * action_density\n",
        "\n",
        "    def local_kineticterm(self, cfgs):\n",
        "        return -2.0 * self.kappa * self.nearest_neighbours_sum(cfgs)\n",
        "\n",
        "    def nearest_neighbours_sum(self, cfgs):\n",
        "        return torch.roll(cfgs, 1, 1) + torch.roll(cfgs, -1, 1) + torch.roll(cfgs, 1, 2) + torch.roll(cfgs, -1, 2)\n",
        "\n",
        "    def delta_local_action(self, x_active, x_frozen, delta, protocol=[1.,1.,1.]):\n",
        "        delta_act = self.phi2term(x_active + delta) - self.phi2term(x_active)\n",
        "        delta_act += self.phi4term(x_active + delta) - self.phi4term(x_active)\n",
        "        delta_act -= 2.0 * self.kappa * delta * self.nearest_neighbours_sum(x_frozen)\n",
        "        return delta_act\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsnYM12Dwuz3"
      },
      "source": [
        "Prior distribution:\n",
        "$$q_0(z)=\\frac{1}{Z_0}e^{-\\sum_i^{|\\Lambda|} \\frac{1}{2\\sigma^2} z_i^2}$$\n",
        "where $Λ$ is a square lattice of size $N_t\\times N_r$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4SqYRGGvvkE"
      },
      "outputs": [],
      "source": [
        "class SimpleNormal(ScalarPhi4Action):\n",
        "    def __init__(self, loc, stand_dev):\n",
        "        ScalarPhi4Action.__init__(self, kappa=0.,lam=0.,phi2coeff=1./(2*stand_dev**2))\n",
        "        self.dist = torch.distributions.normal.Normal(loc, torch.ones_like(loc) * stand_dev)\n",
        "        self.dist_loc = torch.distributions.normal.Normal(0.0, stand_dev)\n",
        "        self.sd = stand_dev\n",
        "        self.logz0_loc = np.log(stand_dev * np.sqrt(2*np.pi))\n",
        "        self.logz0 = len(torch.flatten(loc)) * self.logz0_loc\n",
        "\n",
        "        self.lam = 0\n",
        "        self.kappa = 0\n",
        "        self.phi2coeff = 1.0 / (2.0 * self.sd**2)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        logp = self.dist.log_prob(x)\n",
        "        return torch.sum(logp, dim=(1,2))\n",
        "\n",
        "    def sample_n(self, batch_size):\n",
        "        x = self.dist.sample((batch_size,))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ0UpY8WYJzI"
      },
      "source": [
        "The following class is used to compute the action $S_{\\eta_i}$, the protocol which interpolates between the prior and the target theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyA-l9VzYKs-"
      },
      "outputs": [],
      "source": [
        "class ActionProtocolPhi4:\n",
        "    def __init__(self, action0, action1):\n",
        "        self.dims = range(1,3)\n",
        "        self.action0 = action0\n",
        "        self.action1 = action1\n",
        "        self.N_protocol_par=3\n",
        "    def __call__(self, cfgs, protocol):\n",
        "        return torch.sum(self.action_density(cfgs, protocol), dim=tuple(self.dims))\n",
        "\n",
        "    def action_density(self, cfgs, protocol):\n",
        "        act = self.phi2coeff(protocol[0]) * cfgs**2 + self.phi4coeff(protocol[1]) * cfgs**4 + self.kincoeff(protocol[2]) * cfgs * (torch.roll(cfgs, 1, 1) + torch.roll(cfgs, 1, 2))\n",
        "        return act\n",
        "\n",
        "    def phi2coeff(self, protocol0):\n",
        "        return protocol0 * self.action1.phi2coeff + (1.0 - protocol0) * self.action0.phi2coeff\n",
        "\n",
        "    def phi4coeff(self, protocol1):\n",
        "        return protocol1 * self.action1.lam + (1.0 - protocol1) * self.action0.lam\n",
        "\n",
        "    def kincoeff(self, protocol2):\n",
        "        return -2.0 * (protocol2 * self.action1.kappa + (1.0 - protocol2) * self.action0.kappa)\n",
        "\n",
        "    def phi4term(self, cfgs, protocol1):\n",
        "        act = self.phi4coeff(protocol1) * cfgs**4\n",
        "        return act\n",
        "\n",
        "    def local_kineticterm(self, cfgs, protocol2):\n",
        "        act = self.kincoeff(protocol2) * self.action1.nearest_neighbours_sum(cfgs)\n",
        "        return act\n",
        "\n",
        "    def delta_local_action(self, x_active, x_frozen, delta, protocol):\n",
        "        delta_act = self.phi2coeff(protocol[0]) * ((x_active + delta)**2 - (x_active)**2)\n",
        "        delta_act += self.phi4coeff(protocol[1]) * ((x_active + delta)**4 - (x_active)**4)\n",
        "        delta_act += self.kincoeff(protocol[2]) * delta * self.action1.nearest_neighbours_sum(x_frozen)\n",
        "\n",
        "        return delta_act"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb6qkmepjGxJ"
      },
      "source": [
        "Building block for the affine convolutional coupling layer, see https://arxiv.org/abs/2101.08176"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT6fJX-7wu3g"
      },
      "outputs": [],
      "source": [
        "def make_conv_net(*, hidden_sizes, kernel_size, in_channels, out_channels, use_tanh=False, use_bias=False):\n",
        "    '''\n",
        "    Convolutional Neural Netowrk\n",
        "    hiddens_sizes=[N_filters for hidden layer 1, .... ,N_filters for hidden layer n]\n",
        "    num hidden layers = len(hidden_sizes)\n",
        "    '''\n",
        "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
        "\n",
        "    padding_size = (kernel_size // 2)\n",
        "    net = []\n",
        "    for i in range(len(sizes) - 1):\n",
        "        conv = torch.nn.Conv2d(sizes[i], sizes[i+1], kernel_size, padding=padding_size, stride=1, padding_mode='zeros', bias=use_bias)\n",
        "        #torch.nn.init.xavier_normal_(conv.weight, gain=torch.nn.init.calculate_gain('tanh'))\n",
        "        #torch.nn.init.normal_(conv.weight, std=0.01)\n",
        "        net.append(conv)\n",
        "        if i != len(sizes) - 2:\n",
        "            if (use_tanh):\n",
        "                net.append(torch.nn.Tanh())\n",
        "            else:\n",
        "                net.append(torch.nn.LeakyReLU(0.1))\n",
        "    net[-1].weight.data.zero_()\n",
        "    if use_bias:\n",
        "        net[-1].bias.data.zero_()\n",
        "    net.append(torch.nn.Tanh())\n",
        "    return torch.nn.Sequential(*net)\n",
        "\n",
        "\n",
        "class AffineCoupling(torch.nn.Module):\n",
        "    '''\n",
        "    RealNVP coupling layer\n",
        "    '''\n",
        "    def __init__(self, net, *, mask_shape, mask_parity, z2_equiv=True):\n",
        "        super().__init__()\n",
        "        self.mask = make_checker_mask(mask_shape, mask_parity)\n",
        "        self.net = net\n",
        "        self.z2_equiv=z2_equiv\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_frozen = self.mask * x\n",
        "        x_active = (1 - self.mask) * x\n",
        "\n",
        "        net_out = self.net(x_frozen.unsqueeze(1)).squeeze(1)\n",
        "        scale, shift = net_out[:,0], net_out[:,1]\n",
        "\n",
        "        if (self.z2_equiv):\n",
        "            s = (1 - self.mask) * torch.abs(scale)\n",
        "        else:\n",
        "            s = (1 - self.mask) * scale\n",
        "        t = (1 - self.mask) * shift\n",
        "\n",
        "        fx = (x_active * torch.exp(-s) - t) + x_frozen\n",
        "        axes = range(1,len(s.size()))\n",
        "        logJ = torch.sum(-s, dim=tuple(axes))\n",
        "        return fx, logJ\n",
        "\n",
        "\n",
        "def make_coupling_layers(*, n_layers, lattice_shape, hidden_sizes, kernel_size,z2_equiv=True):\n",
        "    if z2_equiv:\n",
        "        bias=False\n",
        "        tanh=True\n",
        "    else:\n",
        "        bias=True\n",
        "        tanh=False\n",
        "    layers = []\n",
        "    if (n_layers > 0):\n",
        "        for i in range(n_layers):\n",
        "            parity = i % 2\n",
        "            net = make_conv_net(in_channels=1, out_channels=2, hidden_sizes=hidden_sizes, kernel_size=kernel_size, use_tanh=tanh, use_bias=bias)\n",
        "            coupling = AffineCoupling(net, mask_shape=lattice_shape, mask_parity=parity, z2_equiv=z2_equiv)\n",
        "            layers.append(coupling)\n",
        "\n",
        "    return torch.nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "def make_checker_mask(shape, parity):\n",
        "    checker = torch.ones(shape, dtype=torch.uint8) - parity\n",
        "    checker[::2, ::2] = parity\n",
        "    checker[1::2, 1::2] = parity\n",
        "    return checker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzMrBKq2jcxK"
      },
      "source": [
        "## Stochastic Normalizing Flows\n",
        "\n",
        "The second main ingredient of Stochastic Normalizing Flows are is the NE-MCMC algorithm, wich main idea is to exploit the Jarzynski equality:\n",
        "$$\\frac{Z_{\\eta_N}}{Z_{\\eta_{0}}}=\\langle e^{-W}\\rangle_f$$\n",
        "\n",
        "in order to compute the partition function of the generated samples knowing the density of the prior $q_0$, the target un-normalized density $S_{\\eta_N}$ and the heat $Q$ of the transformation:\n",
        "$$W(\\phi)=S_{\\eta_{N}}[\\phi_N]-S_{\\eta_{0}}[\\phi_0]-Q(\\phi_0,\\phi_1,...,\\phi_N)$$\n",
        "$$Q=\\sum_{i=1}^{N-1}\\{S_{\\eta_{i+1}}[\\phi_{i+1}]-S_{\\eta_{i+1}}[\\phi_{i}]\\}$$\n",
        "\n",
        "The update used to implement the stochastic layer is the heatbath algorithm.\n",
        "\n",
        "In the case of Stochastic Normalizing Flows, we interlive stocastic updates with Normalizing Flows coupling layers:\n",
        "$$Z_{\\eta_N}=\\langle e^{-w}\\rangle_f$$\n",
        "where\n",
        "$$W=S_{\\eta_N}[\\phi_N]+\\ln q_0 (\\phi_0)-\\ln|\\det J_g|-Q(\\phi_0,\\phi_1,...,\\phi_N)$$\n",
        "with $\\ln|\\det J_g|$ sum of the log determinat of the Jacobian of the normalizing flow layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK7-KYUSwLcb"
      },
      "source": [
        "### Heatbath\n",
        "If we want to generate a new configuration in Markov chain, we can update the lattice by applying a given algorithm one site $\\phi(x)$ at a time, leaving all the other sites fixed. In this way we perform a so-called sweep of the lattice sites. Since the action contains only nearest-neighbour interaction, one can apply the algorithm for all the even sites (leaving the odd ones fixed) and then the other way around. This is convenient on hardware such as GPUs where it is better to perform as many concurrent operations as possible.\n",
        "The action terms that involve the site $\\phi(x)$ look like this:\n",
        "$$S_{loc}(x)=-2\\kappa \\sum_{\\mu=-2,-1,1,2,}\\phi(x)\\phi(x+\\hat{\\mu})+(1-2\\lambda)\\phi(x)^2+\\lambda\\phi(x)^4=A\\phi(x)+B\\phi(x)^2+C\\phi(x)^4$$\n",
        "where A, B and C are fixed coefficients. Of course A depends on the four nearest neighbours, but these are left fixed in the update of $\\phi(x)$. The rest of the action is unchanged and we can ignore it.\n",
        "The goal here is to propose a new $\\phi'$ from the Boltzmann distribution\n",
        "$$p(\\phi(x))\\sim \\exp(-S_{loc}[\\phi])=\\exp(-A\\phi(x)-B\\phi(x)^2-C\\phi(x)^4)$$\n",
        "using a heatbah algorithm. The general details are contained in Chapter 7 of the [Montvay-Muenster book](https://www.cambridge.org/core/books/quantum-fields-on-a-lattice/4401A88CD232B0AEF1409BF6B260883A) on Lattice QCD, as well as a detailed description of the SU(2) gauge theory case.\n",
        "\n",
        "It is remarkably simple to generate a distribution very similar to p: it is simply a shifted normal distribution\n",
        "$$p_0(\\phi(x))\\sim \\exp(−A\\phi − B\\phi^2) \\sim exp(−By^2)$$\n",
        "with $\\phi=y-\\frac{A}{2B}$. One simply samples y from a normal distribution and then calculates $\\phi$.\n",
        "However, this is not the correct distribution: this proposal for the new value of $\\phi$ needs an accept-reject step to take into account the remaining part of the action, that is the $\\exp(−C\\phi^4)$ extra factor in the probability distribution. We do so by accepting/rejecting the proposal with a probability\n",
        "\n",
        "$$\\min\\biggl( 1,\\frac{p(\\phi)}{p_0(\\phi)}\\biggr) =\\min\\biggl( 1,\\exp(−C\\phi^4)\\biggr)$$\n",
        "\n",
        "In principle one should repeat the proposal + accept/reject steps until a valid candidate is accepted. It is more common to set a maximum number of iterations M to avoid getting stuck on a particular value. If the proposal is rejected M times, the old $\\phi(x)$ remains unchanged. Interestingly, M can be as low as 1 without compromising the essential properties of the algorithm. This is the strategy adopted in this code, in order to improve the efficiency on a GPU, where many concurrent operations are to be preferred than repeating the accept-reject step on a subset of sites. The actual, most efficient value of M depends also on the values of $x$ and $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvJdqjYFw8C9"
      },
      "outputs": [],
      "source": [
        "class MonteCarloUpdate(torch.nn.Module):\n",
        "    def __init__(self, lattice_shape, action_protocol, protocol_pars, nsteps=1, device='cuda'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pars=protocol_pars\n",
        "\n",
        "        self.nsteps = nsteps #Numbers of stochastic updates for one stochastic layer\n",
        "\n",
        "        self.action_protocol = action_protocol\n",
        "        self.N_pars=action_protocol.N_protocol_par\n",
        "        self.action_density = action_protocol.action_density\n",
        "\n",
        "\n",
        "        self.update = self.local_heatbath\n",
        "        self.heatbathSd = 1.0 / np.sqrt(2.0 * self.action_protocol.phi2coeff(self.pars[0]))\n",
        "        self.norm_dist = torch.distributions.normal.Normal(torch.zeros(lattice_shape), self.heatbathSd)\n",
        "        self.unif_dist = torch.distributions.uniform.Uniform(torch.zeros(lattice_shape), torch.ones(lattice_shape))\n",
        "\n",
        "        self.even_mask = make_checker_mask(lattice_shape, 0)\n",
        "        self.odd_mask = make_checker_mask(lattice_shape, 1)\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "    def heatbath(self, x_active, x_frozen, norm_rn, unif_rn):\n",
        "        nn_sum = self.action_protocol.local_kineticterm(x_frozen, self.pars[2])\n",
        "        shift = - self.heatbathSd**2 * nn_sum\n",
        "\n",
        "        #only one proposal: in principle one could generate a new proposal for not-accepted configurations until they are all accepted\n",
        "        #very inefficient on GPUs\n",
        "        x_active_new = norm_rn + shift\n",
        "        accepted = torch.exp(-self.action_protocol.phi4term(x_active_new, self.pars[1])) > unif_rn\n",
        "        x_active = x_active * ~accepted + x_active_new * accepted\n",
        "        return x_active\n",
        "\n",
        "\n",
        "    def local_heatbath(self, x):\n",
        "        S_old = self.action_density(x, self.pars)\n",
        "\n",
        "        for n in range(self.nsteps):\n",
        "\n",
        "            if self.device == 'cuda':\n",
        "                norm_rn = torch.cuda.FloatTensor(x.shape).normal_(0.0, self.heatbathSd)\n",
        "                unif_rn = torch.cuda.FloatTensor(x.shape).uniform_(0.0, 1.0)\n",
        "            else:\n",
        "                norm_rn = self.norm_dist.sample((x.shape[0],))\n",
        "                unif_rn = self.unif_dist.sample((x.shape[0],))\n",
        "\n",
        "            x_odd = x * self.odd_mask\n",
        "            x_even = x * self.even_mask\n",
        "            x_even = self.heatbath(x_even, x_odd, norm_rn * self.even_mask, unif_rn * self.even_mask)\n",
        "            x_odd = self.heatbath(x_odd, x_even, norm_rn * self.odd_mask, unif_rn * self.odd_mask)\n",
        "\n",
        "            x = x_even + x_odd\n",
        "\n",
        "        S = self.action_density(x, self.pars)\n",
        "        dQ = torch.sum(S - S_old, dim=(1,2))\n",
        "\n",
        "        return x, dQ\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, dQ = self.update(x)\n",
        "\n",
        "        return x, dQ\n",
        "\n",
        "def make_stochastic_layers(*, layers, action_protocol, nsteps, lattice_shape, dl_per_sg=2, sl_per_sg=1, input_protocol=None, device='cuda'):\n",
        "    N_protocol_par=action_protocol.N_protocol_par\n",
        "    if len(layers)==0:\n",
        "        n_stoch_groups = 1\n",
        "        n_stoch_layers = sl_per_sg\n",
        "        dl_per_sg = 0\n",
        "    elif dl_per_sg==0:\n",
        "        n_stoch_groups = 1\n",
        "        n_stoch_layers = n_stoch_groups * sl_per_sg\n",
        "    else:\n",
        "        n_layers = len(layers)\n",
        "        #how many deterministic layers per stochastic layer\n",
        "        #dl_per_sg must be even and a factor of n_layers\n",
        "        n_stoch_groups = n_layers//dl_per_sg\n",
        "        n_stoch_layers = n_stoch_groups * sl_per_sg\n",
        "\n",
        "    for i in range(n_stoch_groups):\n",
        "        pos = (i+1)*dl_per_sg + i*sl_per_sg\n",
        "        for j in range(sl_per_sg):\n",
        "            if (input_protocol is None):\n",
        "                protocol_par = (i*sl_per_sg + j + 1)/n_stoch_layers\n",
        "                protocol_pars=np.ones(N_protocol_par)*protocol_par\n",
        "            elif input_protocol=='non-smooth':\n",
        "                protocol_par = (pos + 1)/(n_layers + n_stoch_layers)\n",
        "                protocol_pars=np.ones(N_protocol_par)*protocol_par\n",
        "            else:\n",
        "                protocol_pars=input_protocol[i+1]\n",
        "\n",
        "            coupling = MonteCarloUpdate(lattice_shape, action_protocol, protocol_pars, nsteps=nsteps, device=device)\n",
        "            layers.insert(pos, coupling)\n",
        "            pos += 1\n",
        "    return torch.nn.ModuleList(layers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xpqoeD3O9e"
      },
      "source": [
        "## Stochastic Normalizing Flows\n",
        "The following class can be used to build Normalizing Flows (stochastic=False), Stochastic Evolution (stochastic=True, n_layers=0) and Stochastic Normalizing Flows (stochastic=True, n_layers>0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCIGtrIsFV4C"
      },
      "outputs": [],
      "source": [
        "class Flow:\n",
        "    def __init__(self, prior, action, lattice_shape, n_layers, hidden_sizes, kernel_size=3, z2_equiv=True,\n",
        "                 stochastic=False, protocol_action=None, dl_per_sg=2, sl_per_sg=1, MCnsteps=1, input_protocol=None,\n",
        "                device  = 'cuda'):\n",
        "        self.prior = prior\n",
        "        self.lattice_shape = lattice_shape\n",
        "        self.history = {'loss' : [], 'loss_var' : [], 'ess' : [], 'logZ' : []}\n",
        "        self.z2_equiv = z2_equiv\n",
        "        self.kernel_size = kernel_size\n",
        "        self.action=action\n",
        "\n",
        "        self.layers = make_coupling_layers(z2_equiv=z2_equiv, lattice_shape=lattice_shape, n_layers=n_layers, hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
        "\n",
        "        if stochastic:\n",
        "            self.layers = make_stochastic_layers(layers=self.layers, action_protocol=protocol_action, dl_per_sg=dl_per_sg, nsteps=MCnsteps,\n",
        "                                                 lattice_shape=lattice_shape, sl_per_sg=sl_per_sg, input_protocol=input_protocol, device=device)\n",
        "\n",
        "\n",
        "        self.device = device\n",
        "        if device == 'cuda':\n",
        "            self.layers.to(device)\n",
        "\n",
        "    def __call__(self, batch_size):\n",
        "        z = self.prior.sample_n(batch_size).to(self.device)\n",
        "        logq = self.prior.log_prob(z)\n",
        "        x, Q = self.apply_flow(z, self.layers, batch_size = batch_size)\n",
        "        return x, logq, Q\n",
        "\n",
        "    def apply_flow(self, z, coupling_layers, *, batch_size):\n",
        "        Q = torch.zeros(batch_size)\n",
        "        i = 0\n",
        "        x = z\n",
        "        for layer in coupling_layers:\n",
        "            x, dQ, = layer.forward(x)\n",
        "            Q = Q + dQ\n",
        "            i = i + 1\n",
        "        return x, Q\n",
        "\n",
        "    def compute_metrics(self, w):\n",
        "        loss = w.mean()\n",
        "        w_np = grab(w)\n",
        "        loss_var = w_np.var()\n",
        "        N = len(w_np)\n",
        "        logZ = logsumexp(-w_np)\n",
        "        log_ess = 2.0 * logZ - logsumexp(-2 * w_np)\n",
        "        ess_per_cfg = np.exp(log_ess) / N\n",
        "        logZ = logZ - np.log(N)\n",
        "\n",
        "        return loss, loss_var, ess_per_cfg, -logZ\n",
        "\n",
        "    def print_metrics(self, era, epoch, avg_last_N_epochs, t0):\n",
        "\n",
        "        print(f'== Era {era} | Epoch {epoch} metrics ==')\n",
        "        for key, val in self.history.items():\n",
        "            avgd = np.mean(val[-avg_last_N_epochs:])\n",
        "            print(f'\\t{key} {avgd:g}')\n",
        "        print(time.time()-t0)\n",
        "\n",
        "\n",
        "    def compile(self, base_lr=.001,patience=500, factor=0.92):\n",
        "        self.optimizer = torch.optim.Adam(self.layers.parameters(), lr=base_lr)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', factor=factor, patience=patience, min_lr=1e-07, verbose=True)\n",
        "\n",
        "\n",
        "    def train_step(self, batch_size, use_crooks=False):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        x, logq, Q = self(batch_size=batch_size)\n",
        "\n",
        "        act = self.action(x)\n",
        "        w = act + logq - Q\n",
        "        loss, loss_var, ess, logZ = self.compute_metrics(w)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step(loss)\n",
        "\n",
        "        self.history['loss'].append(grab(loss))\n",
        "        self.history['loss_var'].append(grab(loss_var))\n",
        "        self.history['ess'].append(grab(ess))\n",
        "        self.history['logZ'].append(grab(logZ))\n",
        "\n",
        "\n",
        "    def fit(self, *, N_era, N_epoch, batch_size, verbose=True, live=True):\n",
        "\n",
        "        print(' N_era=' + str(N_era) + '\\n N_epoch=' + str(N_epoch) + '\\n batch_size=' + str(batch_size) + '\\n')\n",
        "\n",
        "        if live:\n",
        "            [plt.close(plt.figure(fignum)) for fignum in plt.get_fignums()] # close all existing figures\n",
        "            live_plot = init_live_plot(N_era*N_epoch)\n",
        "\n",
        "        t0=time.time()\n",
        "        for era in range(N_era):\n",
        "            for epoch in range(N_epoch):\n",
        "                self.train_step(batch_size)\n",
        "                if verbose:\n",
        "                    if epoch % N_epoch == 0:\n",
        "                        self.print_metrics(era, epoch, avg_last_N_epochs=N_epoch, t0=t0)\n",
        "                if live:\n",
        "                    if epoch % 1 == 0:\n",
        "                        update_plots(self.history, **live_plot)\n",
        "\n",
        "\n",
        "        print('Last learning rate = ' + str(self.scheduler._last_lr))\n",
        "\n",
        "        return self.history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WzD5j6x3vsg"
      },
      "source": [
        "Simple test of the training of Stochastic Normalizing Flows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rLcHBfwvSlEH",
        "outputId": "2157158f-c326-4303-a75a-20028c9bf91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AffineCoupling(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): Tanh()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (3): Tanh()\n",
            "    (4): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n",
            "AffineCoupling(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): Tanh()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (3): Tanh()\n",
            "    (4): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " N_era=1000\n",
            " N_epoch=10\n",
            " batch_size=100\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Era 0 | Epoch 0 metrics ==\n",
            "\tloss -148.762\n",
            "\tloss_var 31.5444\n",
            "\tess 0.034992\n",
            "\tlogZ -156.333\n",
            "0.46685361862182617\n",
            "== Era 1 | Epoch 0 metrics ==\n",
            "\tloss -158.613\n",
            "\tloss_var 19.8608\n",
            "\tess 0.0253649\n",
            "\tlogZ -166.764\n",
            "3.4578607082366943\n",
            "== Era 2 | Epoch 0 metrics ==\n",
            "\tloss -167.809\n",
            "\tloss_var 13.7673\n",
            "\tess 0.0261465\n",
            "\tlogZ -173.633\n",
            "5.732755184173584\n",
            "== Era 3 | Epoch 0 metrics ==\n",
            "\tloss -167.844\n",
            "\tloss_var 13.3737\n",
            "\tess 0.0289528\n",
            "\tlogZ -174.146\n",
            "8.063147783279419\n",
            "== Era 4 | Epoch 0 metrics ==\n",
            "\tloss -168.527\n",
            "\tloss_var 14.1394\n",
            "\tess 0.0329505\n",
            "\tlogZ -174.695\n",
            "10.351672649383545\n",
            "== Era 5 | Epoch 0 metrics ==\n",
            "\tloss -168.708\n",
            "\tloss_var 13.7947\n",
            "\tess 0.0301059\n",
            "\tlogZ -174.704\n",
            "12.560707807540894\n",
            "== Era 6 | Epoch 0 metrics ==\n",
            "\tloss -168.627\n",
            "\tloss_var 13.1734\n",
            "\tess 0.0340949\n",
            "\tlogZ -174.393\n",
            "15.29367470741272\n"
          ]
        }
      ],
      "source": [
        "Ns = 16  #Spatial extension\n",
        "Nt = 16 #Temporal extension\n",
        "lattice_shape = (Nt,Ns)\n",
        "n_layers = 24   #number of affine coupling layers ( 1 affine block=2 affine coupling layers)\n",
        "hidden_sizes = [32,32] #hidden size of the convolutional network (we are using just the output layers)\n",
        "kernel_size = 3  #Kernel size\n",
        "N_era = 1000     #one Era correspond to N epoch\n",
        "N_epoch = 10    #Numbers of gradient updates for one Era\n",
        "batch_size = 100 #batch size for one epoch\n",
        "base_lr = .0001 #initial learning rate\n",
        "sl_per_sg = 1 # # of stochastic layer for stochastic group / #of steps of SE when n_layers=0\n",
        "\n",
        "\n",
        "#target action\n",
        "k=0.27\n",
        "lam=0.022\n",
        "action=ScalarPhi4Action(k,lam)\n",
        "#prior\n",
        "sd=math.sqrt(1.0/(4.0*lam+2))\n",
        "prior = SimpleNormal(torch.zeros(lattice_shape),sd)\n",
        "#protocol action\n",
        "protocol_action=ActionProtocolPhi4(prior,action)\n",
        "#flow class\n",
        "flow=Flow(stochastic=False, sl_per_sg=sl_per_sg, MCnsteps=1, dl_per_sg=2,\n",
        "          prior=prior, lattice_shape=lattice_shape,kernel_size=kernel_size, n_layers=n_layers,\n",
        "          hidden_sizes=hidden_sizes, z2_equiv=True,\n",
        "          action=action, protocol_action=protocol_action,input_protocol='non-smooth', device=torch_device)\n",
        "\n",
        "print(flow.layers[0]) #print the first coupling layer\n",
        "print(flow.layers[2]) #print the first stochastic layer\n",
        "\n",
        "flow.compile(0.0001) #we use ADAM as optimizer and ReduceLROnPlateau scheduler\n",
        "#Training:\n",
        "history=flow.fit(N_era=N_era, N_epoch=N_epoch, batch_size = batch_size, verbose=True, live=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs00NNNGf1sR"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVwcGXSUuH2q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}